{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy.util import load_model\n",
    "\n",
    "from src.config import PARAM, PATH_MODEL, PATH_DATA_I, PATH_DATA_P\n",
    "from src.pd_helpers import normalize_cols, aggregate\n",
    "from src.spacy_helpers import fetch_docs\n",
    "from src.doc_analysis import basic_stats\n",
    "\n",
    "batches = PARAM.batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = load_model(PATH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for batch in batches:\n",
    "    df = pd.read_pickle(PATH_DATA_I / f'{batch}_.pkl')\n",
    "    pre_df = list()\n",
    "\n",
    "    for doc in fetch_docs(PATH_DATA_P / batch, nlp.vocab):\n",
    "        stats = basic_stats(doc)   \n",
    "        pre_df.append(stats)\n",
    "\n",
    "    df_doc = pd.DataFrame(pre_df)\n",
    "    df_doc.columns = [col.lower() for col in df_doc.columns]\n",
    "    df_doc['n_words'] = df_doc['n_tokens'] - df_doc['n_stopwords']\n",
    "\n",
    "    normalizations = [\n",
    "        ('n_words',     'n_tokens',    True),\n",
    "        ('n_entities',  'n_tokens',    True),\n",
    "        ('n_sentences', 'n_tokens',    True),\n",
    "        ('pos',         'n_tokens',    True),\n",
    "        ('ent',         'n_tokens',    True),\n",
    "        ('uniq_ent',    'n_entities',  True),\n",
    "        ]\n",
    "    \n",
    "    for n in normalizations:\n",
    "        cols = [col for col in df_doc.columns if col.startswith(n[0])]\n",
    "        df_doc = normalize_cols(df_doc, cols, n[1], invert=n[2])\n",
    "\n",
    "    df_ = df.merge(df_doc, left_index=True, right_index=True)\n",
    "    df_all = df_all.append(df_, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'n_sentences',\n",
    "    'n_tokens',\n",
    "    'n_stopwords',\n",
    "    'n_words',\n",
    "    'n_entities',\n",
    "    'n_tokens/n_sentences',\n",
    "    'n_tokens/n_words',\n",
    "    'n_tokens/n_entities']\n",
    "aggregate(df_all, cols, 'source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_tokens']\n",
    "cols.extend(sorted([col for col in df_all.columns if 'n_tokens/' in col and 'pos_' in col]))\n",
    "aggregate(df_all, cols, 'source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_entities']\n",
    "cols.extend(sorted([col for col in df_all.columns if 'n_entities/' in col and 'ent_' in col]))\n",
    "aggregate(df_all, cols, 'source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_tokens', 'n_entities']\n",
    "cols.extend(sorted([col for col in df_all.columns if 'n_tokens/' in col and 'ent_' in col]))\n",
    "aggregate(df_all, cols, 'source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1 = 'n_entities/'\n",
    "crit2 = 'place'\n",
    "cols = ['n_tokens', 'n_entities']\n",
    "cols.extend(sorted([col for col in df_all.columns if crit1 in col and crit2 in col]))\n",
    "df_avg_ent = df_all.groupby(['source', 'section'])[cols].agg(['mean']).round(1)\n",
    "df_avg_ent.columns = [f\"avg:{col[0]}\" for col in df_avg_ent.columns.values]\n",
    "df_avg_ent = df_avg_ent.join(df_all.groupby(['source', 'section'])['section'].count())\n",
    "df_avg_ent.query('section >= 10').sort_values(f\"avg:{cols[2]}\"\n",
    "    ).sort_index(level='source', sort_remaining=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1 = 'n_tokens/'\n",
    "crit2 = 'place'\n",
    "cols = ['n_tokens', 'n_entities']\n",
    "cols.extend(sorted([col for col in df_all.columns if crit1 in col and crit2 in col]))\n",
    "df_avg_ent = df_all.groupby(['source', 'section'])[cols].agg(['mean']).round(1)\n",
    "df_avg_ent.columns = [f\"avg:{col[0]}\" for col in df_avg_ent.columns.values]\n",
    "df_avg_ent = df_avg_ent.join(df_all.groupby(['source', 'section'])['section'].count())\n",
    "df_avg_ent.query('section >= 10').sort_values(f\"avg:{cols[2]}\"\n",
    "    ).sort_index(level='source', sort_remaining=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "place-extraction",
   "language": "python",
   "name": "place-extraction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
